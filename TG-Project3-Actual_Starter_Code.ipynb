{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.11</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests, time, json, datetime, dill, pixiedust\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_boardgames = \"http://www.reddit.com/r/boardgames.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send a request to reddit getting the first 25 posts\n",
    "res = requests.get(URL_boardgames, headers = {'User-agent': 'project3 Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see that there's stuff there. Don't worry, there is.\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting more data\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = data['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_boardgames+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    data['data']['children'].extend(temp_data['data']['children'])\n",
    "    data['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['data']['children']) #not sure why there are 251 results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data with the two subreddits I chose: r/TalesFromTechSupport and r/LFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these work but they are early iterations on the Data-Gathering-Script (other file), which does the scraping while I can work on the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#Tales From Tech Support Data Scraping\n",
    "URL_tfts = \"http://www.reddit.com/r/talesfromtechsupport.json\"\n",
    "\n",
    "res = requests.get(URL_tfts, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['tfts'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['tfts'['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_tfts+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['tfts']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['tfts']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['tfts']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#LFG Data Scraping\n",
    "\n",
    "URL_lfg = \"http://www.reddit.com/r/LFG.json\"\n",
    "\n",
    "res = requests.get(URL_lfg, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['lfg'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['lfg']['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_lfg+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['lfg']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['lfg']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['lfg']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {'tfts':data_tfts, 'lfg':data_lfg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#Create a backup of my data in case the current working data is overridden\n",
    "#only run occasionally, usually while figuring out how to append json files together\n",
    "import json, datetime\n",
    "\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/backup_my_data_' + k + str(datetime.datetime.now()) + '.json'\n",
    "    with open(filepath, 'w+') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#writing to a json file in my project\n",
    "import json\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/my_data_' + k + '.json'\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#reading from a json file in my project\n",
    "import json\n",
    "jsons = {}\n",
    "for i in ['tfts', 'lfg']:\n",
    "    with open('./data/my_data_'+i+'.json', 'r') as f:\n",
    "        jsons[i] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess I didn't need to save the json files...\n",
    "\n",
    "What do I want to be in the features? Obviously the text from the post but I'm going to take a look at some of the other features that are given to us from the json file.\n",
    "\n",
    "Subreddits I chose:\n",
    " - https://www.reddit.com/r/talesfromtechsupport\n",
    " - https://www.reddit.com/r/lfg/\n",
    " \n",
    "Other options:\n",
    " - https://www.reddit.com/r/dataisbeautiful/\n",
    " - https://www.reddit.com/r/airz23  \n",
    " - https://www.reddit.com/r/nosleep\n",
    "\n",
    "_wanna make it really hard? pick airz and tfts_\n",
    "\n",
    "Potential features:\n",
    "- `'subreddit'`\n",
    "- `'url'`\n",
    "- `'author'`\n",
    "- `'domain'`\n",
    "- `'downs'`\n",
    "- `'is_self'` \n",
    "- `'is_video'` \n",
    "- `'likes'`\n",
    "- `'media'`\n",
    "- `'num_comments'`\n",
    "- `'num_crossposts'`\n",
    "- `'num_reports'`\n",
    "- `'selftext'`\n",
    "- `'score'`\n",
    "- `'title'`\n",
    "- `'ups'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_path = './data/main_dataframe.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2782, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in main data if not already in memory\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "try:\n",
    "    main_df = pd.read_csv(main_data_path) \n",
    "    #this is for the initial scrape, when we dont have a df saved as a csv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in new data and add to current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2715, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in freshly scraped data\n",
    "new_data_path = './data/new_data.csv'\n",
    "\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3161, 16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add to dataset and delete duplicates\n",
    "main_df = main_df.append(new_df, ignore_index=True)\n",
    "main_df.drop_duplicates(subset=['url'], inplace=True)\n",
    "main_df.reset_index(drop=True, inplace=True)\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save main df \n",
    "main_df.to_csv(main_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup of main df\n",
    "#run every so often just so if we accidentally overwrite main_df save we still have our data\n",
    "import datetime\n",
    "\n",
    "filepath = './data/backup_my_dataframe_'+ str(datetime.datetime.now())+'.csv'\n",
    "main_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to save the state of the notebook so if I have to relaunch I don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill_session = '083018_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 11.8 ms, total: 128 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dill\n",
    "# Save\n",
    "dill.dump_session('project3_notebook_env_'+ dill_session +'.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dill\n",
    "# Load\n",
    "dill.load_session('project3_notebook_env_' + dill_session + '.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the posts\n",
    "\n",
    "I need to make sure that the posts aren't giving away where they're from in any obvious way, such as having the subreddit name in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases that indicate subreddit name - to remove\n",
    "phrases = ['tales from tech support', 'looking for gamers', 'looking for games', 'no sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/r/lfg',\n",
       " '/r/nosleep',\n",
       " '/r/talesfromtechsupport',\n",
       " 'lfg',\n",
       " 'nosleep',\n",
       " 'r/lfg',\n",
       " 'r/nosleep',\n",
       " 'r/talesfromtechsupport',\n",
       " 'talesfromtechsupport',\n",
       " 'tfts'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = set()\n",
    "for i in ['talesfromtechsupport', 'lfg', 'nosleep']:\n",
    "    words = [i, '/r/'+i, 'r/'+i]\n",
    "    my_words.update(words)\n",
    "\n",
    "words = ['tfts']\n",
    "my_words.update(words)\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'exampl text entir sure paid $49.23 hey /r/dumb_th great place 39% comput comput comput wonder happen'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input\n",
    "    # text - string\n",
    "    # stem_or_lem - string, 'stem' uses PorterStemmer, 'lem' uses WordNetLemmatizer, anything else uses nothing\n",
    "    \n",
    "# Output\n",
    "    # a string\n",
    "    \n",
    "def process(text, stem_or_lem = 'lem', stop = 'english'):\n",
    "    \n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove potential phrases of the subreddit name\n",
    "    phrases = ['tales from tech support', 'looking for gamers', 'looking for games', 'no sleep']\n",
    "    for i in phrases:\n",
    "        text = text.replace(i, '')\n",
    "\n",
    "    \n",
    "    # Grab all of the words. Disregard punctuation. \n",
    "    tokenizer = RegexpTokenizer(r'(\\$?(\\d+[\\.,]?)+%?|(\\/?\\w+)+)') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #print(tokens)\n",
    "    new_tokens = [i[0] for i in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    if not stop==None:\n",
    "        #print('Stop')\n",
    "        stops = set(stopwords.words(stop))\n",
    "        stops.update(my_words)\n",
    "        new_tokens = [word for word in new_tokens if not word in stops]\n",
    "    \n",
    "    if stem_or_lem == 'lem':\n",
    "        #print('Lem')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        new_tokens = [lemmatizer.lemmatize(i) for i in new_tokens]\n",
    "        \n",
    "    elif stem_or_lem == 'stem':\n",
    "        #print('Stem')\n",
    "        p_stemmer = PorterStemmer()\n",
    "        new_tokens = [p_stemmer.stem(i) for i in new_tokens]\n",
    "        \n",
    "   \n",
    "    \n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "text = \"This is an example of some text! I'm NoT eNtIrElY sure why I paid $49.23 for it but hey, /r/dumb_thing /r/lfg r/LFG tales from tech support tfts looking for gamers is a great place. 39%. computers compute computing. I.Wonder.What'll.Happen\"\n",
    "\n",
    "results = process(text, 'stem')\n",
    "print(type(results))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit          object\n",
       "url                object\n",
       "author             object\n",
       "domain             object\n",
       "downs               int64\n",
       "is_self              bool\n",
       "is_video             bool\n",
       "likes             float64\n",
       "media             float64\n",
       "num_comments        int64\n",
       "num_crossposts      int64\n",
       "num_reports       float64\n",
       "selftext           object\n",
       "score               int64\n",
       "title              object\n",
       "ups                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in ['title', 'selftext']:\n",
    "    main_df[i] = main_df[i].map(lambda x: process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "pixieapp_metadata": null
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pixie_debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subreddit', 'url', 'author', 'domain', 'downs', 'is_self', 'is_video',\n",
      "       'likes', 'media', 'num_comments', 'num_crossposts', 'num_reports',\n",
      "       'selftext', 'score', 'title', 'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
