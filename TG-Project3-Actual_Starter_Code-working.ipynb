{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data Gathering](#data_gathering)\n",
    "\n",
    "[Data Saving](#data_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust \n",
    "# Sometimes this is helpful, sometimes it breaks everything. Use with caution.\n",
    "# Imported for the pixie_debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json, datetime, dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_boardgames = \"http://www.reddit.com/r/boardgames.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send a request to reddit getting the first 25 posts\n",
    "res = requests.get(URL_boardgames, headers = {'User-agent': 'project3 Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see that there's stuff there. Don't worry, there is.\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n"
     ]
    }
   ],
   "source": [
    "#collecting more data\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = data['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_boardgames+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    data['data']['children'].extend(temp_data['data']['children'])\n",
    "    data['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['data']['children']) #not sure why there are 251 results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data with the two subreddits I chose: r/TalesFromTechSupport and r/BuildAPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Please see [Data-Gathering-Script.ipynb](Data-Gathering-Script.ipynb) for the most recent version of the data gathering code. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#Tales From Tech Support Data Scraping\n",
    "URL_tfts = \"http://www.reddit.com/r/talesfromtechsupport.json\"\n",
    "\n",
    "res = requests.get(URL_tfts, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['tfts'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['tfts']['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_tfts+URL_EXTENDER+last_title, headers = {'User-agent':'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['tfts']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['tfts']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['tfts']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#LFG Data Scraping\n",
    "\n",
    "URL_lfg = \"http://www.reddit.com/r/LFG.json\"\n",
    "\n",
    "res = requests.get(URL_lfg, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['lfg'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['lfg']['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_lfg+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['lfg']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['lfg']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['lfg']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jsons = {'tfts':data_tfts, 'lfg':data_lfg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a backup of my data in case the current working data is overridden\n",
    "#only run occasionally, usually while figuring out how to append json files together\n",
    "import json, datetime\n",
    "\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/backup_my_data_' + k + str(datetime.datetime.now()) + '.json'\n",
    "    with open(filepath, 'w+') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to a json file in my project\n",
    "import json\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/my_data_' + k + '.json'\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from a json file in my project\n",
    "import json\n",
    "jsons = {}\n",
    "for i in ['tfts', 'lfg']:\n",
    "    with open('./data/my_data_'+i+'.json', 'r') as f:\n",
    "        jsons[i] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do I want to be in the features? Obviously the text from the post but I'm going to take a look at some of the other features that are given to us from the json file.\n",
    "\n",
    "Subreddits I chose:\n",
    " - https://www.reddit.com/r/talesfromtechsupport\n",
    " - https://www.reddit.com/r/buildapc/\n",
    " \n",
    "Other options:\n",
    " - https://www.reddit.com/r/dataisbeautiful/\n",
    " - https://www.reddit.com/r/airz23  \n",
    " - https://www.reddit.com/r/nosleep\n",
    " - https://www.reddit.com/r/lfg/\n",
    "\n",
    "_wanna make it really hard? pick airz and tfts_\n",
    "\n",
    "Potentially useful features:\n",
    "- `'subreddit'`\n",
    "- `'url'`\n",
    "- `'author'`\n",
    "- `'domain'`\n",
    "- `'downs'`\n",
    "- `'is_self'` \n",
    "- `'is_video'` \n",
    "- `'likes'`\n",
    "- `'media'`\n",
    "- `'num_comments'`\n",
    "- `'num_crossposts'`\n",
    "- `'num_reports'`\n",
    "- `'selftext'`\n",
    "- `'score'`\n",
    "- `'title'`\n",
    "- `'ups'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_gathering\"></a>\n",
    "Data Gathering: Create main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_path = './data/main_dataframe.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4312, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in main data \n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "try:\n",
    "    main_df = pd.read_csv(main_data_path) \n",
    "    #this is for the initial scrape, when we dont have a df saved as a csv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in new data and add to current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971, 16)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in freshly scraped data\n",
    "new_data_path = './data/new_data.csv'\n",
    "\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4853, 16)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add to dataset and delete duplicates\n",
    "main_df = main_df.append(new_df, ignore_index=True)\n",
    "main_df.drop_duplicates(subset=['url'], inplace=True)\n",
    "main_df.reset_index(drop=True, inplace=True)\n",
    "main_df['selftext'] = main_df['selftext'].replace(np.nan, '')\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_saving\"></a>\n",
    "\n",
    "Saving our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save main df \n",
    "main_df.to_csv(main_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup of main df\n",
    "#run every so often just so if we accidentally overwrite main_df save we still have our data\n",
    "import datetime\n",
    "\n",
    "filepath = './data/backup_my_dataframe_'+ str(datetime.datetime.now())+'.csv'\n",
    "main_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is to save the state of the notebook so if I have to relaunch I don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a session id instead of the \n",
    "dill_session = '090518_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.5 s, sys: 1.22 s, total: 35.7 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save\n",
    "import dill\n",
    "\n",
    "dill.dump_session('./data/project3_notebook_env_'+ dill_session +'.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 574 ms, total: 2.37 s\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load\n",
    "import dill\n",
    "\n",
    "dill.load_session('./data/project3_notebook_env_' + dill_session + '.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the posts\n",
    "\n",
    "I need to make sure that the posts aren't giving away where they're from in any obvious way, such as having the subreddit name in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/r/buildapc',\n",
       " '/r/lfg',\n",
       " '/r/nosleep',\n",
       " '/r/talesfromtechsupport',\n",
       " 'buildapc',\n",
       " 'lfg',\n",
       " 'nosleep',\n",
       " 'r/buildapc',\n",
       " 'r/lfg',\n",
       " 'r/nosleep',\n",
       " 'r/talesfromtechsupport',\n",
       " 'talesfromtechsupport',\n",
       " 'tfts',\n",
       " 'www.reddit.com/r/buildapc',\n",
       " 'www.reddit.com/r/lfg',\n",
       " 'www.reddit.com/r/nosleep',\n",
       " 'www.reddit.com/r/talesfromtechsupport'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = set()\n",
    "for i in ['talesfromtechsupport', 'lfg', 'nosleep', 'buildapc']:\n",
    "    words = [i, '/r/'+i, 'r/'+i, 'www.reddit.com/r/'+i]\n",
    "    my_words.update(words)\n",
    "\n",
    "my_words.update(['tfts'])\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'exampl text entir sure paid $49.23 hey /r/dumb_th great place 39% comput comput comput wonder happen'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Input\n",
    "    # text - string\n",
    "    # stem_or_lem - string, 'stem' uses PorterStemmer, 'lem' uses WordNetLemmatizer, anything else uses nothing\n",
    "    \n",
    "# Output\n",
    "    # a string\n",
    "    \n",
    "def process(text, stem_or_lem = 'lem', stop = 'english'):\n",
    "    \"\"\"\n",
    "    Function that: Sets text to lowercase; Removes direct indications of a subreddit's name; Removes stop words; Stems, lemmatizes, or neither; \n",
    "    \n",
    "    :param text: The body of text to process.\n",
    "    :Example: \"This is a sentence. It has to do with reddit. Specifically, r/talesfromtechsupport\"\n",
    "    \n",
    "    :param stem_or_lem: Option to stem, lemmatize, or neither\n",
    "    :Example: 'stem'\n",
    "    :Example: 'lem'\n",
    "    :Example: 'snow'\n",
    "    :Example: 'Any other text for no'\n",
    "    \n",
    "    :param stop: What language of NLTK stopwords to use in addition to the ones that are pre-made\n",
    "    :Example: 'english'\n",
    "    \n",
    "    :return: String of post-processed text.\n",
    "    \"\"\" \n",
    "    \n",
    "    try:\n",
    "        if len(text)==0:\n",
    "            return text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove potential phrases of the subreddit name\n",
    "    phrases = ['tales from tech support', 'looking for gamers', 'looking for games', 'no sleep', 'build a pc']\n",
    "    for i in phrases:\n",
    "        text = text.replace(i, '')\n",
    "\n",
    "    \n",
    "    # Grab all of the words. Disregard punctuation. \n",
    "    tokenizer = RegexpTokenizer(r'(\\$?(\\d+[\\.,]?)+%?|(\\/?\\w+)+)') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #print(tokens)\n",
    "    new_tokens = [i[0] for i in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    if not stop==None:\n",
    "        #print('Stop')\n",
    "        stops = set(stopwords.words(stop))\n",
    "        stops.update(my_words)\n",
    "        new_tokens = [word for word in new_tokens if not word in stops]\n",
    "    \n",
    "    if stem_or_lem == 'lem':\n",
    "        #print('Lem')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        new_tokens = [lemmatizer.lemmatize(i) for i in new_tokens]\n",
    "        \n",
    "    elif stem_or_lem == 'stem':\n",
    "        #print('Stem')\n",
    "        p_stemmer = PorterStemmer()\n",
    "        new_tokens = [p_stemmer.stem(i) for i in new_tokens]\n",
    "        \n",
    "    elif stem_or_lem == 'snow':\n",
    "        s_stemmer = EnglishStemmer()\n",
    "        new_tokens = [s_stemmer.stem(i) for i in new_tokens]\n",
    "        \n",
    "   \n",
    "    \n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "text = \"This is an example of some text! I'm NoT eNtIrElY sure why I paid $49.23 for it but hey, /r/dumb_thing /r/lfg r/LFG tales from tech support tfts looking for gamers is a great place. 39%. computers compute computing. I.Wonder.What'll.Happen\"\n",
    "\n",
    "results = process(text, 'stem')\n",
    "print(type(results))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(process(None, 'stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(process(np.nan, 'stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit          object\n",
       "url                object\n",
       "author             object\n",
       "domain             object\n",
       "downs               int64\n",
       "is_self              bool\n",
       "is_video             bool\n",
       "likes             float64\n",
       "media             float64\n",
       "num_comments        int64\n",
       "num_crossposts      int64\n",
       "num_reports       float64\n",
       "selftext           object\n",
       "score               int64\n",
       "title              object\n",
       "ups                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df['selftext'] = main_df['selftext'].replace(np.nan, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subreddit', 'url', 'author', 'domain', 'downs', 'is_self', 'is_video',\n",
      "       'likes', 'media', 'num_comments', 'num_crossposts', 'num_reports',\n",
      "       'selftext', 'score', 'title', 'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe that I'm going to work with. My 'main' dataframe has more than 2 subreddits worth of data\n",
    "\n",
    "subreddit1 = 'talesfromtechsupport'\n",
    "subreddit2 = 'buildapc'\n",
    "\n",
    "working_df = main_df[(main_df['subreddit']==subreddit1)|(main_df['subreddit']==subreddit2)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2534, 16)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing data cleaning and processing \n",
    "\n",
    "\n",
    "for i in ['title', 'selftext']:\n",
    "    working_df[i] = working_df[i].map(lambda x: process(x, 'stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the target into 0 & 1\n",
    "working_df['subreddit'] = working_df['subreddit'].map({subreddit1:1, subreddit2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>MagicBigfoot</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey two stickies something like 90% mod remova...</td>\n",
       "      <td>1958</td>\n",
       "      <td>posting rules mobile users please read</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>ArmaSwiss</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>work telecom data/phones/cctv dental office cl...</td>\n",
       "      <td>879</td>\n",
       "      <td>amp took client internet busy time 45 minutes</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>kanersps</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hello everyone posted already r/choosingbegger...</td>\n",
       "      <td>31</td>\n",
       "      <td>free laptop good enough</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>Mr_White119811</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened whilst served armed forces whilst spe...</td>\n",
       "      <td>122</td>\n",
       "      <td>clusterf k sandpit</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>sambeaux45</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best anonymize story best amp x200b tale goes ...</td>\n",
       "      <td>1944</td>\n",
       "      <td>time providing hours support may saved life</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                                url  \\\n",
       "0          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "4          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "           author                     domain  downs  is_self  is_video  likes  \\\n",
       "0    MagicBigfoot  self.talesfromtechsupport      0     True     False    NaN   \n",
       "1       ArmaSwiss  self.talesfromtechsupport      0     True     False    NaN   \n",
       "2        kanersps  self.talesfromtechsupport      0     True     False    NaN   \n",
       "3  Mr_White119811  self.talesfromtechsupport      0     True     False    NaN   \n",
       "4      sambeaux45  self.talesfromtechsupport      0     True     False    NaN   \n",
       "\n",
       "   media  num_comments  num_crossposts  num_reports  \\\n",
       "0    NaN            77               0          NaN   \n",
       "1    NaN            91               0          NaN   \n",
       "2    NaN             5               0          NaN   \n",
       "3    NaN            15               0          NaN   \n",
       "4    NaN           202               0          NaN   \n",
       "\n",
       "                                            selftext  score  \\\n",
       "0  hey two stickies something like 90% mod remova...   1958   \n",
       "1  work telecom data/phones/cctv dental office cl...    879   \n",
       "2  hello everyone posted already r/choosingbegger...     31   \n",
       "3  happened whilst served armed forces whilst spe...    122   \n",
       "4  best anonymize story best amp x200b tale goes ...   1944   \n",
       "\n",
       "                                           title   ups  \n",
       "0         posting rules mobile users please read  1958  \n",
       "1  amp took client internet busy time 45 minutes   879  \n",
       "2                        free laptop good enough    31  \n",
       "3                             clusterf k sandpit   122  \n",
       "4    time providing hours support may saved life  1944  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the target and selecting some key features\n",
    "y = working_df['subreddit']\n",
    "X = working_df[['title', 'selftext', 'score', 'downs', 'ups', 'num_comments']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature (the main feature I am going to use) that combines the title and body of a post\n",
    "X['both_texts'] = X['title'] + ' ' + X['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ended up using just the title and body of the post, all in 1 string, as the only feature in X. I might add in the rest of the numeric features at a later point\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['both_texts'], y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,5))\n",
    "X_train_trans = cv.fit_transform(X_train)\n",
    "X_test_trans = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 5.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.3, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=5,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.3, penalty='l1', n_jobs=5, random_state=42)\n",
    "lr.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779559118236473"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_trans, y_test) \n",
    "# That's uh .. that's pretty good.\n",
    "# Makes me feel like there are some key words I might've missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998661311914324"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 ngrams = 0.9939024\n",
    "# 1 ngrams = 0.9939024390243902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.53509355e-06, 9.99993465e-01],\n",
       "       [9.99985284e-01, 1.47160685e-05],\n",
       "       [2.09846474e-03, 9.97901535e-01],\n",
       "       [1.00000000e+00, 9.48731562e-16],\n",
       "       [1.66875130e-04, 9.99833125e-01],\n",
       "       [9.98919817e-01, 1.08018331e-03],\n",
       "       [1.00000000e+00, 1.56711689e-13],\n",
       "       [1.46584851e-06, 9.99998534e-01],\n",
       "       [9.90466160e-01, 9.53384019e-03],\n",
       "       [9.98558314e-01, 1.44168585e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X_test_trans)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({'Features':cv.get_feature_names(),\n",
    "                        'coef':lr.coef_[0],\n",
    "                        'Absolute Coefficient':np.abs(lr.coef_[0])})\n",
    "\n",
    "temp_df.sort_values('Absolute Coefficient', inplace=True, ascending=False)\n",
    "temp_df[temp_df['Absolute Coefficient']>0].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>coef</th>\n",
       "      <th>Absolute Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2410969</th>\n",
       "      <td>tech</td>\n",
       "      <td>0.767830</td>\n",
       "      <td>0.767830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360414</th>\n",
       "      <td>support</td>\n",
       "      <td>0.753583</td>\n",
       "      <td>0.753583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608697</th>\n",
       "      <td>user</td>\n",
       "      <td>0.622415</td>\n",
       "      <td>0.622415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856446</th>\n",
       "      <td>problem</td>\n",
       "      <td>0.570529</td>\n",
       "      <td>0.570529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269196</th>\n",
       "      <td>body</td>\n",
       "      <td>-0.544598</td>\n",
       "      <td>0.544598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454177</th>\n",
       "      <td>computer</td>\n",
       "      <td>0.499051</td>\n",
       "      <td>0.499051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766471</th>\n",
       "      <td>eye</td>\n",
       "      <td>-0.497189</td>\n",
       "      <td>0.497189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218140</th>\n",
       "      <td>issue</td>\n",
       "      <td>0.460203</td>\n",
       "      <td>0.460203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532220</th>\n",
       "      <td>customer</td>\n",
       "      <td>0.402755</td>\n",
       "      <td>0.402755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341035</th>\n",
       "      <td>life</td>\n",
       "      <td>-0.372816</td>\n",
       "      <td>0.372816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753060</th>\n",
       "      <td>working</td>\n",
       "      <td>0.368264</td>\n",
       "      <td>0.368264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112264</th>\n",
       "      <td>server</td>\n",
       "      <td>0.320006</td>\n",
       "      <td>0.320006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814635</th>\n",
       "      <td>felt</td>\n",
       "      <td>-0.313940</td>\n",
       "      <td>0.313940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446305</th>\n",
       "      <td>company</td>\n",
       "      <td>0.311532</td>\n",
       "      <td>0.311532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624493</th>\n",
       "      <td>night</td>\n",
       "      <td>-0.294232</td>\n",
       "      <td>0.294232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743921</th>\n",
       "      <td>work</td>\n",
       "      <td>0.289504</td>\n",
       "      <td>0.289504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351798</th>\n",
       "      <td>like</td>\n",
       "      <td>-0.283740</td>\n",
       "      <td>0.283740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775980</th>\n",
       "      <td>face</td>\n",
       "      <td>-0.277436</td>\n",
       "      <td>0.277436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127663</th>\n",
       "      <td>around</td>\n",
       "      <td>-0.275957</td>\n",
       "      <td>0.275957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348772</th>\n",
       "      <td>subject</td>\n",
       "      <td>-0.268186</td>\n",
       "      <td>0.268186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Features      coef  Absolute Coefficient\n",
       "2410969      tech  0.767830              0.767830\n",
       "2360414   support  0.753583              0.753583\n",
       "2608697      user  0.622415              0.622415\n",
       "1856446   problem  0.570529              0.570529\n",
       "269196       body -0.544598              0.544598\n",
       "454177   computer  0.499051              0.499051\n",
       "766471        eye -0.497189              0.497189\n",
       "1218140     issue  0.460203              0.460203\n",
       "532220   customer  0.402755              0.402755\n",
       "1341035      life -0.372816              0.372816\n",
       "2753060   working  0.368264              0.368264\n",
       "2112264    server  0.320006              0.320006\n",
       "814635       felt -0.313940              0.313940\n",
       "446305    company  0.311532              0.311532\n",
       "1624493     night -0.294232              0.294232\n",
       "2743921      work  0.289504              0.289504\n",
       "1351798      like -0.283740              0.283740\n",
       "775980       face -0.277436              0.277436\n",
       "127663     around -0.275957              0.275957\n",
       "2348772   subject -0.268186              0.268186"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[temp_df['Absolute Coefficient']>0].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scrolling past the next cell's output](#past_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning type stem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed: 52.5min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed: 54.6min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed: 57.3min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 60.6min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 64.2min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 68.5min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 72.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4354.768s\n",
      "\n",
      "Best score: 0.993\n",
      "Best parameters set:\n",
      "\tmodel__C: 0.05\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 2\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type stem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 23.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1390.060s\n",
      "\n",
      "Best score: 0.994\n",
      "Best parameters set:\n",
      "\tmodel__C: 5.882758620689654\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 2\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type stem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 431.089s\n",
      "\n",
      "Best score: 0.967\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.1\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type stem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 453.212s\n",
      "\n",
      "Best score: 0.977\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.05\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type lem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 25.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1512.638s\n",
      "\n",
      "Best score: 0.994\n",
      "Best parameters set:\n",
      "\tmodel__C: 0.05\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 2\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type lem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 24.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1471.412s\n",
      "\n",
      "Best score: 0.995\n",
      "Best parameters set:\n",
      "\tmodel__C: 5.882758620689654\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 3\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type lem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   50.8s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 458.924s\n",
      "\n",
      "Best score: 0.967\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.05\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type lem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 478.087s\n",
      "\n",
      "Best score: 0.979\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.05\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type not stem or lem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 26.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1578.251s\n",
      "\n",
      "Best score: 0.994\n",
      "Best parameters set:\n",
      "\tmodel__C: 0.05\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 2\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type not stem or lem.\n",
      "\n",
      "Model: logistic_regression\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__penalty': ['l1', 'l2'],\n",
       " 'model__random_state': [42],\n",
       " 'model__C': array([ 0.05      ,  0.39310345,  0.7362069 ,  1.07931034,  1.42241379,\n",
       "         1.76551724,  2.10862069,  2.45172414,  2.79482759,  3.13793103,\n",
       "         3.48103448,  3.82413793,  4.16724138,  4.51034483,  4.85344828,\n",
       "         5.19655172,  5.53965517,  5.88275862,  6.22586207,  6.56896552,\n",
       "         6.91206897,  7.25517241,  7.59827586,  7.94137931,  8.28448276,\n",
       "         8.62758621,  8.97068966,  9.3137931 ,  9.65689655, 10.        ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=5)]: Done 4860 out of 4860 | elapsed: 25.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1526.174s\n",
      "\n",
      "Best score: 0.995\n",
      "Best parameters set:\n",
      "\tmodel__C: 10.0\n",
      "\tmodel__penalty: 'l2'\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type not stem or lem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: count_vectorizer\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 485.445s\n",
      "\n",
      "Best score: 0.968\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.05\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\n",
      "\n",
      "\n",
      "Cleaning type not stem or lem.\n",
      "\n",
      "Model: multinomial_nb\n",
      "\n",
      "Vectorizer: tfidf\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'model']\n",
      "parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': [1.0, 0.75, 0.5],\n",
       " 'vect__min_df': [1, 2, 3],\n",
       " 'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       " 'model__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "        0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=5)]: Done 1620 out of 1620 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 644.762s\n",
      "\n",
      "Best score: 0.979\n",
      "Best parameters set:\n",
      "\tmodel__alpha: 0.05\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 32s, sys: 1min 33s, total: 5min 6s\n",
      "Wall time: 4h 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Beginnings of figuring out how to test multiple different vectorizers and models at once\n",
    "\n",
    "\n",
    "best_runs = defaultdict(dict)\n",
    "# [stem/lem/none] [logistic/multinomial] [count/tfidf] [score/params]\n",
    "\n",
    "\n",
    "#Running 12 gridsearches * num_fits per model \n",
    "# 6 logreg * 4860 fits = \n",
    "# 6 multinomial * fits = \n",
    "for sl in ['stem', 'lem', 'not stem or lem']:\n",
    "    \n",
    "\n",
    "    #remaking the df to be able to change the way I clean it\n",
    "    subreddit1 = 'talesfromtechsupport'\n",
    "    subreddit2 = 'buildapc'\n",
    "\n",
    "    working_df2 = main_df[(main_df['subreddit']==subreddit1)|(main_df['subreddit']==subreddit2)].copy()\n",
    "\n",
    "    working_df2['subreddit'] = working_df2['subreddit'].map({subreddit1:1, subreddit2:0})\n",
    "    working_df2['selftext'] = working_df2['selftext'].replace(np.nan, '')\n",
    "                          \n",
    "    working_df2['both_texts'] = working_df2['title'] + ' ' + working_df2['selftext']\n",
    "    working_df2['both_texts'] = working_df2['both_texts'].map(lambda x: process(x, sl))\n",
    "    \n",
    "    X2 = working_df2['both_texts']\n",
    "    y2 = working_df2['subreddit']\n",
    "    \n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, stratify = y2, random_state = 42)\n",
    "    \n",
    "    \n",
    "    #testing the models\n",
    "    for model_k, model_v in {'logistic_regression':LogisticRegression(), 'multinomial_nb':MultinomialNB()}.items():\n",
    "                          \n",
    "        #testing the vectorizers\n",
    "        for vectorizer_k, vectorizer_v in {'count_vectorizer':CountVectorizer(), 'tfidf':TfidfVectorizer()}.items():\n",
    "            print(f'Cleaning type {sl}.') \n",
    "            print()\n",
    "            print(f'Model: {model_k}')\n",
    "            print()\n",
    "            print(f'Vectorizer: {vectorizer_k}')\n",
    "            print()\n",
    "\n",
    "            parameters = { #27\n",
    "                'vect__max_df':[1.0, 0.75, 0.5],\n",
    "                'vect__min_df':[1, 2, 3], \n",
    "                'vect__ngram_range':[(1,1), (1,2), (1,3)], #, (1,4), (1,5), (1,6), (1,7)],\n",
    "                \n",
    "                #'model__n_jobs':[5],\n",
    "                #'model__random_state':[42]\n",
    "                #'model__penalty':['l1'] #, 'l2']\n",
    "\n",
    "            }\n",
    "            if(model_k == 'logistic_regression'):\n",
    "                #60\n",
    "                parameters['model__penalty']=['l1', 'l2']\n",
    "                parameters['model__random_state']=[42]\n",
    "                parameters['model__C'] = np.linspace(0.05, 10, 30)\n",
    "\n",
    "            elif(model_k == 'multinomial_nb'):\n",
    "                parameters['model__alpha'] = np.linspace(0.05, 1, 20)\n",
    "\n",
    "                \n",
    "            pipeline = Pipeline([\n",
    "                ('vect', vectorizer_v),\n",
    "                ('model', model_v)\n",
    "            ])\n",
    "\n",
    "            grid_search = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=5)\n",
    "\n",
    "            print(\"Performing grid search...\")\n",
    "            print(\"Pipeline:\", [name for name, _ in pipeline.steps])\n",
    "            print(\"Parameters:\")\n",
    "            display(parameters)\n",
    "            t0 = time.time()\n",
    "            grid_search.fit(X_train2, y_train2)\n",
    "            print(\"Done in %0.3fs\" % (time.time() - t0))\n",
    "            print()\n",
    "\n",
    "            print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "            print(\"Best parameters set:\")\n",
    "            best_parameters = grid_search.best_estimator_.get_params()\n",
    "            for param_name in sorted(parameters.keys()):\n",
    "                print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                best_runs[sl][model_k] = {}\n",
    "                best_runs[sl][model_k][vectorizer_k] = {}\n",
    "                best_runs[sl][model_k][vectorizer_k]['params'] = best_parameters[param_name]\n",
    "                best_runs[sl][model_k][vectorizer_k]['score'] = grid_search.best_score_\n",
    "            print('\\n\\n')\n",
    "        print('\\n\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"past_output\"></a>\n",
    "Past Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score 0.9953145917001339\n",
      "Best Params (1, 1)\n",
      "Best Vectorizer tfidf\n",
      "Best Model logistic_regression\n",
      "Best Cleaning not stem or lem\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_params = None\n",
    "best_vectorizer = None\n",
    "best_model = None\n",
    "best_sl = None\n",
    "for sl_k, sl_v in best_runs.items():\n",
    "    for model_k, model_v in sl_v.items():\n",
    "        for vect_k, vect_v in model_v.items():\n",
    "            if vect_v['score'] > best_score:\n",
    "                best_score = vect_v['score']\n",
    "                best_params = vect_v['params']\n",
    "                best_vectorizer = vect_k\n",
    "                best_model = model_k\n",
    "                best_sl = sl_k\n",
    "\n",
    "print('Best Score', best_score)\n",
    "print('Best Params', best_params)\n",
    "print('Best Vectorizer', best_vectorizer)\n",
    "print('Best Model', best_model)\n",
    "print('Best Cleaning', best_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for reference in case the data gets reset.\n",
    "\n",
    "Best Score 0.9953145917001339\n",
    "\n",
    "Best Params (1, 1)\n",
    "\n",
    "Best Vectorizer tfidf\n",
    "\n",
    "Best Model logistic_regression\n",
    "\n",
    "Best Cleaning not stem or lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "source": [
    "\n",
    "I already encoded them for the previous models. Specifically, this is the code that encoded them, copied from above. \n",
    "```python\n",
    "subreddit1 = 'talesfromtechsupport'\n",
    "subreddit2 = 'buildapc'\n",
    "\n",
    "working_df['subreddit'] = working_df['subreddit'].map(\n",
    "    {subreddit1:1, \n",
    "     subreddit2:0}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.6029992107340174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1528\n",
       "1    1006\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The majority class / all observations\n",
    "# This assumes that we are predicting every outcome as the majority class\n",
    "counts = working_df['subreddit'].value_counts()\n",
    "print('Baseline Accuracy:', counts[0] / (counts[0]+counts[1]))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Val for Random Forest: 0.9594731080445794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=5,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, n_jobs = 5, random_state=42)\n",
    "print(\"Cross-Val for Random Forest:\", cross_val_score(rf, X_train_trans, y_train).mean())\n",
    "rf.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589905362776026"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_trans, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Val for Extra Trees: 0.9568426350910242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=5,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=5)\n",
    "print('Cross-Val for Extra Trees:', cross_val_score(et, X_train_trans, y_train).mean())\n",
    "et.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9526813880126183"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.score(X_test_trans, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "    - I don't think that there needs to be any other metric than Accuracy. We don't have a preference for minimizing false negatives or false positives.\n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testing_grounds(stem_lem_options, model_options, model_params, vectorizer_options, vectorizer_params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that takes in multiple parawwmeters for cleaning type, vectorizer, model, and hyperparameter tuning and tests all possible combinations.\n",
    "    \n",
    "    :param stem_lem_options: a list of data cleaning options consisting of some combination of ['stem', 'lem', 'snow', 'none']\n",
    "    \n",
    "    :param model_options: a dictionary of models to test.\n",
    "    :Example: \n",
    "    {'logistic_regression':LogisticRegression(), 'multinomial_nb':MultinomialNB()}\n",
    "    \n",
    "    :param model_params: a dictionary of model parameters to test.\n",
    "    :Example: {'model_name':{ 'param_name':[param options] }}\n",
    "    \n",
    "    :param vectorizer_options: a dictionary of vectorizers to test.\n",
    "    :Example: {'count_vectorizer':CountVectorizer(), 'tfidf':TfidfVectorizer()}\n",
    "    \n",
    "    :param vectorizer_params: a dictionary of model parameters and the model they pertain to.\n",
    "    :Example: {'model_name':{ 'param_name':[param options] }}\n",
    "    \n",
    "    :return: dictionary 4 levels deep with all passed options as keys and the best parameters and scores for each combination.\n",
    "    :Example: best_runs[stem_lem_options][model_options][vectorizer_options]['score','params']\n",
    "    \"\"\"\n",
    "    num_grids = len(stem_lem_options)*len(model_options)*len(vectorizer_options)\n",
    "    print(\"Total Number of Gridsearches:\", num_grids)\n",
    "    index=1\n",
    "    best_runs = defaultdict(dict)\n",
    "    # [stem/lem/none] [logistic/multinomial] [count/tfidf] [score/params]\n",
    "\n",
    "\n",
    "    for sl in stem_lem_options:\n",
    "\n",
    "        #remaking the df to be able to change the way I clean it\n",
    "        subreddit1 = 'talesfromtechsupport'\n",
    "        subreddit2 = 'buildapc'\n",
    "\n",
    "        working_df2 = main_df[(main_df['subreddit']==subreddit1)|(main_df['subreddit']==subreddit2)].copy()\n",
    "\n",
    "        working_df2['subreddit'] = working_df2['subreddit'].map({subreddit1:1, subreddit2:0})\n",
    "        working_df2['selftext'] = working_df2['selftext'].replace(np.nan, '')\n",
    "\n",
    "        working_df2['both_texts'] = working_df2['title'] + ' ' + working_df2['selftext']\n",
    "        working_df2['both_texts'] = working_df2['both_texts'].map(lambda x: process(x, sl))\n",
    "\n",
    "        X2 = working_df2['both_texts']\n",
    "        y2 = working_df2['subreddit']\n",
    "\n",
    "        X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, stratify = y2, random_state = 42)\n",
    "\n",
    "\n",
    "        #testing the models\n",
    "        for model_k, model_v in model_options.items():\n",
    "\n",
    "            #testing the vectorizers\n",
    "            for vectorizer_k, vectorizer_v in vectorizer_options.items():\n",
    "\n",
    "                parameters = {}\n",
    "                \n",
    "                for params_k, params_v in model_params.items():\n",
    "                    if params_k == model_k:\n",
    "                        for param_k, param_v in params_v.items():\n",
    "                            parameters['model__'+param_k] = param_v\n",
    "                \n",
    "                for params_k, params_v in vectorizer_params.items():\n",
    "                    if params_k == vectorizer_k:\n",
    "                        for param_k, param_v in params_v.items():\n",
    "                            parameters['vect__'+param_k] = param_v\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('vect', vectorizer_v),\n",
    "                    ('model', model_v)\n",
    "                ])\n",
    "\n",
    "                grid_search = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=3)\n",
    "\n",
    "                print(f\"Performing grid search #{index} of {num_grids}...\")\n",
    "                index+=1\n",
    "                print(f\"Pipeline: {sl}, {model_k}, {vectorizer_k}\\n\")\n",
    "                print(\"Parameters:\")\n",
    "                display(parameters)\n",
    "                t0 = time.time()\n",
    "                grid_search.fit(X_train2, y_train2)\n",
    "                print(\"Done in %0.3fs\" % (time.time() - t0))\n",
    "                print()\n",
    "\n",
    "                print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "                print(\"Best parameters set:\")\n",
    "                best_parameters = grid_search.best_estimator_.get_params()\n",
    "                for param_name in sorted(parameters.keys()):\n",
    "                    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                    best_runs[sl][model_k] = {}\n",
    "                    best_runs[sl][model_k][vectorizer_k] = {}\n",
    "                    best_runs[sl][model_k][vectorizer_k]['params'] = best_parameters\n",
    "                    best_runs[sl][model_k][vectorizer_k]['score'] = grid_search.best_score_\n",
    "                print('\\n\\n')\n",
    "            print('\\n\\n\\n\\n\\n')\n",
    "            \n",
    "    return best_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Gridsearches: 4\n",
      "Performing grid search #1 of 4...\n",
      "Pipeline: stem, extra_trees, tfidf\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [150, 200, 250],\n",
       " 'model__criterion': ['gini'],\n",
       " 'model__random_state': [42],\n",
       " 'vect__max_df': [3, 4, 5, 1.0],\n",
       " 'vect__min_df': [3, 2],\n",
       " 'vect__ngram_range': [(1, 1)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:   39.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 40.616s\n",
      "\n",
      "Best score: 0.995\n",
      "Best parameters set:\n",
      "\tmodel__criterion: 'gini'\n",
      "\tmodel__n_estimators: 200\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 3\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #2 of 4...\n",
      "Pipeline: lem, extra_trees, tfidf\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [150, 200, 250],\n",
       " 'model__criterion': ['gini'],\n",
       " 'model__random_state': [42],\n",
       " 'vect__max_df': [3, 4, 5, 1.0],\n",
       " 'vect__min_df': [3, 2],\n",
       " 'vect__ngram_range': [(1, 1)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:   41.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 43.333s\n",
      "\n",
      "Best score: 0.994\n",
      "Best parameters set:\n",
      "\tmodel__criterion: 'gini'\n",
      "\tmodel__n_estimators: 200\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 3\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #3 of 4...\n",
      "Pipeline: snow, extra_trees, tfidf\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [150, 200, 250],\n",
       " 'model__criterion': ['gini'],\n",
       " 'model__random_state': [42],\n",
       " 'vect__max_df': [3, 4, 5, 1.0],\n",
       " 'vect__min_df': [3, 2],\n",
       " 'vect__ngram_range': [(1, 1)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:   39.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 40.920s\n",
      "\n",
      "Best score: 0.993\n",
      "Best parameters set:\n",
      "\tmodel__criterion: 'gini'\n",
      "\tmodel__n_estimators: 250\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 3\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Performing grid search #4 of 4...\n",
      "Pipeline: none, extra_trees, tfidf\n",
      "\n",
      "Parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model__n_estimators': [150, 200, 250],\n",
       " 'model__criterion': ['gini'],\n",
       " 'model__random_state': [42],\n",
       " 'vect__max_df': [3, 4, 5, 1.0],\n",
       " 'vect__min_df': [3, 2],\n",
       " 'vect__ngram_range': [(1, 1)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:   41.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 42.565s\n",
      "\n",
      "Best score: 0.993\n",
      "Best parameters set:\n",
      "\tmodel__criterion: 'gini'\n",
      "\tmodel__n_estimators: 150\n",
      "\tmodel__random_state: 42\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__min_df: 2\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 23.1 s, sys: 1.13 s, total: 24.3 s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#This is testing all combinations of [Porter Stemming, Snowball Stemming, Lemmatizing, and None], [Logistic Regression, MultinomialNB, RandomForest, and ExtraTrees] and [CountVectorizer and TF-IDF]\n",
    "\n",
    "\n",
    "stem_lem_options = ['stem', 'lem', 'snow', 'none']\n",
    "\n",
    "model_options = {\n",
    "#     'logistic_regression':LogisticRegression(), \n",
    "#     'multinomial_nb':MultinomialNB(),\n",
    "#     'random_forest':RandomForestClassifier(),\n",
    "    'extra_trees':ExtraTreesClassifier(),\n",
    "#     'ada':AdaBoostClassifier(),\n",
    "#     'gradient':GradientBoostingClassifier(),\n",
    "#     voting':VotingClassifier({\n",
    "#            'vote_ada':AdaBoostClassifier(),\n",
    "#            'vote_gradient':GradientBoostingClassifier()\n",
    "#        })\n",
    "}\n",
    "\n",
    "model_params = { \n",
    "#     'logistic_regression':\n",
    "#     {\n",
    "#         'penalty':['l1', 'l2'],\n",
    "#         'random_state':[42],\n",
    "#         'C':[.9, 1., 1.1]\n",
    "#     },\n",
    "    \n",
    "#     'multinomial_nb':\n",
    "#     {\n",
    "#         'alpha':[0.9, 1., 1.1]\n",
    "#     },\n",
    "    \n",
    "#     'random_forest':\n",
    "#     {\n",
    "#         'n_estimators':[10, 50, 100, 200],\n",
    "#         'criterion':['gini', 'entropy'],\n",
    "#         'random_state':[42]\n",
    "#     },\n",
    "    \n",
    "    'extra_trees':\n",
    "    {\n",
    "        'n_estimators':[150, 200, 250,],# 300], #[10, 50, 100, 200],\n",
    "        'criterion':['gini'], #, 'entropy'],\n",
    "        'random_state':[42]\n",
    "    },\n",
    "    \n",
    "#     'ada':\n",
    "#     {\n",
    "#         'n_estimators':[40,50,60],\n",
    "#         'random_state':[42],\n",
    "#         'learning_rate':[.9,1.,1.1]\n",
    "#     },\n",
    "    \n",
    "#     'gradient':\n",
    "#     {\n",
    "#         'n_estimators':[90,100,110],\n",
    "#         'learning_rate':[.05,.1,.15,.2],\n",
    "#         'random_state':[42]\n",
    "#     },\n",
    "    \n",
    "#     'voting':\n",
    "#     {\n",
    "#         'voting':['hard', 'soft']\n",
    "#     }\n",
    "}\n",
    "\n",
    "vectorizer_options = {\n",
    "    #'count_vectorizer':CountVectorizer(),\n",
    "    #'hashing_vectorizer':HashingVectorizer(), # This just seems to break everything\n",
    "    'tfidf':TfidfVectorizer()\n",
    "}\n",
    "\n",
    "vectorizer_params = { # This has some overlap between vectorizer parameters. I could add an 'all' section but that's over working it\n",
    "#     'count_vectorizer':\n",
    "#     {\n",
    "#         'max_df':[1.0, 0.75, 0.5],\n",
    "#         'min_df':[1, 2, 3], \n",
    "#         'ngram_range':[(1,1)]#, (1,2), (1,3)]\n",
    "#     },\n",
    "    \n",
    "#     'hashing_vectorizer':\n",
    "#     {\n",
    "#         'ngram_range':[(1,1), (1,2), (1,3)]\n",
    "#     },\n",
    "    \n",
    "    'tfidf':\n",
    "    {\n",
    "        'max_df': [3, 4, 5, 1.], #[3.0, 2.0, 1.5, 1.3], #, 1.3, 1.2, 1.1, 1.0,],#  0.75, 0.5],\n",
    "        'min_df':[3, 2], #1, 2, 3], \n",
    "        'ngram_range':[(1,1)]#, (1,2), (1,3)]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_runs = testing_grounds(stem_lem_options, model_options, model_params, vectorizer_options, vectorizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:       0.9947368421052631\n",
      "Best Params:     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=3.0, max_features=None, min_df=3,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('model',\n",
       "   ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "              oob_score=False, random_state=42, verbose=0, warm_start=False))],\n",
       " 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=3.0, max_features=None, min_df=3,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'model': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 3.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 3,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__norm': 'l2',\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__smooth_idf': True,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__sublinear_tf': False,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__use_idf': True,\n",
       " 'vect__vocabulary': None,\n",
       " 'model__bootstrap': False,\n",
       " 'model__class_weight': None,\n",
       " 'model__criterion': 'gini',\n",
       " 'model__max_depth': None,\n",
       " 'model__max_features': 'auto',\n",
       " 'model__max_leaf_nodes': None,\n",
       " 'model__min_impurity_decrease': 0.0,\n",
       " 'model__min_impurity_split': None,\n",
       " 'model__min_samples_leaf': 1,\n",
       " 'model__min_samples_split': 2,\n",
       " 'model__min_weight_fraction_leaf': 0.0,\n",
       " 'model__n_estimators': 200,\n",
       " 'model__n_jobs': 1,\n",
       " 'model__oob_score': False,\n",
       " 'model__random_state': 42,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Vectorizer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tfidf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model:      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'extra_trees'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Cleaning:    stem\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_params = None\n",
    "best_vectorizer = None\n",
    "best_model = None\n",
    "best_sl = None\n",
    "for sl_k, sl_v in best_runs.items():\n",
    "    for model_k, model_v in sl_v.items():\n",
    "        for vect_k, vect_v in model_v.items():\n",
    "            if vect_v['score'] > best_score:\n",
    "                best_score = vect_v['score']\n",
    "                best_params = vect_v['params']\n",
    "                best_vectorizer = vect_k\n",
    "                best_model = model_k\n",
    "                best_sl = sl_k\n",
    "\n",
    "print('Best Score:      ', best_score)\n",
    "print('Best Params:     ')\n",
    "display(best_params)\n",
    "print()\n",
    "print('Best Vectorizer: ')\n",
    "display(best_vectorizer)\n",
    "print()\n",
    "print('Best Model:      ')\n",
    "display(best_model)\n",
    "print()\n",
    "print('Best Cleaning:   ', best_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = best_params['steps'][1][1]\n",
    "model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `testing_grounds` function above builds different models and tests them.  It returns the best scoring classifier, vectorizer, and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build of the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params['steps'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=3.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params['steps'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926310211417931\n",
      "1.0\n",
      "0.9936908517350158\n"
     ]
    }
   ],
   "source": [
    "subreddit1 = 'talesfromtechsupport'\n",
    "subreddit2 = 'buildapc'\n",
    "\n",
    "working_df3 = main_df[(main_df['subreddit']==subreddit1)|(main_df['subreddit']==subreddit2)].copy()\n",
    "\n",
    "working_df3['subreddit'] = working_df3['subreddit'].map({subreddit1:1, subreddit2:0})\n",
    "working_df3['selftext'] = working_df3['selftext'].replace(np.nan, '')\n",
    "\n",
    "working_df3['both_texts'] = working_df3['title'] + ' ' + working_df3['selftext']\n",
    "working_df3['both_texts'] = working_df3['both_texts'].map(lambda x: process(x, 'stem'))\n",
    "\n",
    "X3 = working_df3['both_texts']\n",
    "y3 = working_df3['subreddit']\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, stratify = y3, random_state = 42)\n",
    "\n",
    "vectorizer3 = best_params['steps'][0][1]\n",
    "X_train3_trans = vectorizer3.fit_transform(X_train3)\n",
    "X_test3_trans = vectorizer3.transform(X_test3)\n",
    "\n",
    "model3 = ExtraTreesClassifier(n_estimators=200, random_state=42)\n",
    "#best_params['steps'][1][1]\n",
    "print(cross_val_score(model3, X_train3_trans, y_train3).mean())\n",
    "model3.fit(X_train3_trans, y_train3)\n",
    "print(model3.score(X_train3_trans, y_train3))\n",
    "print(model3.score(X_test3_trans, y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01273059428180271"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['Importance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>gt</td>\n",
       "      <td>0.012731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>oh</td>\n",
       "      <td>0.012181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>call</td>\n",
       "      <td>0.010876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>stori</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>email</td>\n",
       "      <td>0.009058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>com</td>\n",
       "      <td>0.008792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>minut</td>\n",
       "      <td>0.007573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>phone</td>\n",
       "      <td>0.007478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>http</td>\n",
       "      <td>0.007449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>work</td>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>build</td>\n",
       "      <td>0.007377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>day</td>\n",
       "      <td>0.007294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>game</td>\n",
       "      <td>0.007147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>compani</td>\n",
       "      <td>0.007141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>happen</td>\n",
       "      <td>0.006968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>told</td>\n",
       "      <td>0.006390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>offic</td>\n",
       "      <td>0.006132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>back</td>\n",
       "      <td>0.006115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>gb</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>user</td>\n",
       "      <td>0.005615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features  Importance\n",
       "2373       gt    0.012731\n",
       "3435       oh    0.012181\n",
       "1074     call    0.010876\n",
       "4635    stori    0.010808\n",
       "1847    email    0.009058\n",
       "1284      com    0.008792\n",
       "3169    minut    0.007573\n",
       "3639    phone    0.007478\n",
       "2571     http    0.007449\n",
       "5369     work    0.007394\n",
       "1024    build    0.007377\n",
       "1537      day    0.007294\n",
       "2239     game    0.007147\n",
       "1307  compani    0.007141\n",
       "2439   happen    0.006968\n",
       "4924     told    0.006390\n",
       "3428    offic    0.006132\n",
       "797      back    0.006115\n",
       "2251       gb    0.005965\n",
       "5139     user    0.005615"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'Features':vectorizer3.get_feature_names(),\n",
    "                        'Importance':model3.feature_importances_\n",
    "                        })\n",
    "\n",
    "temp_df.sort_values('Importance', inplace=True, ascending=False)\n",
    "temp_df[temp_df['Importance']>0].head(20)\n",
    "# I learned here that I don't know which word indicates which subreddit when using ExtraTreesClassifier\n",
    "# All of the coefs are >= 0, unlike in LogisticRegression where a positive coef meant one thing and a negative coef meant the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used for the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2534, 17)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1528\n",
       "1    1006\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df3['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "\n",
    "#model3.fit(X_train3_trans[:, np.newaxis], y_train3)\n",
    "\n",
    "dot_data = StringIO()  \n",
    "\n",
    "export_graphviz(model3.estimators_[0], out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>gt</td>\n",
       "      <td>0.012731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>oh</td>\n",
       "      <td>0.012181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>call</td>\n",
       "      <td>0.010876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>stori</td>\n",
       "      <td>0.010808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>email</td>\n",
       "      <td>0.009058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>com</td>\n",
       "      <td>0.008792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>minut</td>\n",
       "      <td>0.007573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>phone</td>\n",
       "      <td>0.007478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>http</td>\n",
       "      <td>0.007449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>work</td>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>build</td>\n",
       "      <td>0.007377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>day</td>\n",
       "      <td>0.007294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>game</td>\n",
       "      <td>0.007147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>compani</td>\n",
       "      <td>0.007141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>happen</td>\n",
       "      <td>0.006968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>told</td>\n",
       "      <td>0.006390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>offic</td>\n",
       "      <td>0.006132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>back</td>\n",
       "      <td>0.006115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>gb</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>user</td>\n",
       "      <td>0.005615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features  Importance\n",
       "2373       gt    0.012731\n",
       "3435       oh    0.012181\n",
       "1074     call    0.010876\n",
       "4635    stori    0.010808\n",
       "1847    email    0.009058\n",
       "1284      com    0.008792\n",
       "3169    minut    0.007573\n",
       "3639    phone    0.007478\n",
       "2571     http    0.007449\n",
       "5369     work    0.007394\n",
       "1024    build    0.007377\n",
       "1537      day    0.007294\n",
       "2239     game    0.007147\n",
       "1307  compani    0.007141\n",
       "2439   happen    0.006968\n",
       "4924     told    0.006390\n",
       "3428    offic    0.006132\n",
       "797      back    0.006115\n",
       "2251       gb    0.005965\n",
       "5139     user    0.005615"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({'Features':vectorizer3.get_feature_names(),\n",
    "                        'Importance':model3.feature_importances_\n",
    "                        })\n",
    "\n",
    "temp_df.sort_values('Importance', inplace=True, ascending=False)\n",
    "temp_df[temp_df['Importance']>0].head(20)\n",
    "# I learned here that I don't know which word indicates which subreddit when using ExtraTreesClassifier\n",
    "# All of the coefs are >= 0, unlike in LogisticRegression where a positive coef meant one thing and a negative coef meant the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=2)\n",
    "plot = sns.barplot(x='Importance', y='Features', data=temp_df.head(10), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.get_figure().savefig('maybe.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df3['subreddit'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfts 352   build 532\n"
     ]
    }
   ],
   "source": [
    "#gt is part of a popular model number\n",
    "gt_tfts = 0\n",
    "gt_build = 0\n",
    "\n",
    "for row in range(working_df3.shape[0]):\n",
    "    if 'gt' in working_df3.iloc[row, 16]:\n",
    "        if working_df3.iloc[row, 0] == 0:\n",
    "            gt_build += 1\n",
    "            #print(working_df3.iloc[row, 16])\n",
    "        else:\n",
    "            gt_tfts += 1\n",
    "\n",
    "print('tfts', gt_tfts, '  build', gt_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfts 859   build 335\n"
     ]
    }
   ],
   "source": [
    "oh_tfts = 0\n",
    "oh_build = 0\n",
    "\n",
    "for row in range(working_df3.shape[0]):\n",
    "    if 'work' in working_df3.iloc[row, 16]:\n",
    "        if working_df3.iloc[row, 0] == 0:\n",
    "            oh_build += 1\n",
    "            #print(working_df3.iloc[row, 16])\n",
    "        else:\n",
    "            oh_tfts += 1\n",
    "\n",
    "print('tfts', oh_tfts, '  build', oh_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
