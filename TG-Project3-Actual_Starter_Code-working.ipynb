{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json, datetime, dill, pixiedust\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_boardgames = \"http://www.reddit.com/r/boardgames.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## send a request to reddit getting the first 25 posts\n",
    "res = requests.get(URL_boardgames, headers = {'User-agent': 'project3 Bot 0.1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see that there's stuff there. Don't worry, there is.\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting more data\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = data['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_boardgames+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    data['data']['children'].extend(temp_data['data']['children'])\n",
    "    data['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['data']['children']) #not sure why there are 251 results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data with the two subreddits I chose: r/TalesFromTechSupport and r/LFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these work but they are early iterations on the Data-Gathering-Script (other file), which does the scraping while I can work on the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#Tales From Tech Support Data Scraping\n",
    "URL_tfts = \"http://www.reddit.com/r/talesfromtechsupport.json\"\n",
    "\n",
    "res = requests.get(URL_tfts, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['tfts'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['tfts'['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_tfts+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['tfts']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['tfts']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['tfts']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#LFG Data Scraping\n",
    "\n",
    "URL_lfg = \"http://www.reddit.com/r/LFG.json\"\n",
    "\n",
    "res = requests.get(URL_lfg, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "jsons['lfg'] = res.json()\n",
    "\n",
    "URL_EXTENDER = \"?after=\"\n",
    "\n",
    "\n",
    "for i in range(9): \n",
    "    #makes a total of 10 requests, or 250 posts. We'll see how much I really need/collect    \n",
    "    #okay so I got 251 posts. Not sure how but guess it doesn't really matter?\n",
    "    last_title = jsons['lfg']['data']['after']\n",
    "    \n",
    "    #retrieve new data\n",
    "    temp_data = requests.get(URL_lfg+URL_EXTENDER+last_title, headers = {'User-agent': 'project3 Bot 0.1'})\n",
    "    \n",
    "    temp_data = temp_data.json()\n",
    "    jsons['lfg']['data']['children'].extend(temp_data['data']['children'])\n",
    "    jsons['lfg']['data']['after'] = temp_data['data']['after']\n",
    "    time.sleep(3)\n",
    "    print('Iteration', i+1)\n",
    "len(jsons['lfg']['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {'tfts':data_tfts, 'lfg':data_lfg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#Create a backup of my data in case the current working data is overridden\n",
    "#only run occasionally, usually while figuring out how to append json files together\n",
    "import json, datetime\n",
    "\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/backup_my_data_' + k + str(datetime.datetime.now()) + '.json'\n",
    "    with open(filepath, 'w+') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#writing to a json file in my project\n",
    "import json\n",
    "\n",
    "for k, v in jsons.items():\n",
    "    filepath = './data/my_data_' + k + '.json'\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(v, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is deprecated. Using the Data-Gathering-Script notebook\n",
    "#reading from a json file in my project\n",
    "import json\n",
    "jsons = {}\n",
    "for i in ['tfts', 'lfg']:\n",
    "    with open('./data/my_data_'+i+'.json', 'r') as f:\n",
    "        jsons[i] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess I didn't need to save the json files...\n",
    "\n",
    "What do I want to be in the features? Obviously the text from the post but I'm going to take a look at some of the other features that are given to us from the json file.\n",
    "\n",
    "Subreddits I chose:\n",
    " - https://www.reddit.com/r/talesfromtechsupport\n",
    " - https://www.reddit.com/r/lfg/\n",
    " \n",
    "Other options:\n",
    " - https://www.reddit.com/r/dataisbeautiful/\n",
    " - https://www.reddit.com/r/airz23  \n",
    " - https://www.reddit.com/r/nosleep\n",
    "\n",
    "_wanna make it really hard? pick airz and tfts_\n",
    "\n",
    "Potential features:\n",
    "- `'subreddit'`\n",
    "- `'url'`\n",
    "- `'author'`\n",
    "- `'domain'`\n",
    "- `'downs'`\n",
    "- `'is_self'` \n",
    "- `'is_video'` \n",
    "- `'likes'`\n",
    "- `'media'`\n",
    "- `'num_comments'`\n",
    "- `'num_crossposts'`\n",
    "- `'num_reports'`\n",
    "- `'selftext'`\n",
    "- `'score'`\n",
    "- `'title'`\n",
    "- `'ups'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_path = './data/main_dataframe.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3161, 16)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in main data \n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "try:\n",
    "    main_df = pd.read_csv(main_data_path) \n",
    "    #this is for the initial scrape, when we dont have a df saved as a csv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in new data and add to current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2715, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in freshly scraped data\n",
    "new_data_path = './data/new_data.csv'\n",
    "\n",
    "new_df = pd.read_csv(new_data_path)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3161, 16)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add to dataset and delete duplicates\n",
    "main_df = main_df.append(new_df, ignore_index=True)\n",
    "main_df.drop_duplicates(subset=['url'], inplace=True)\n",
    "main_df.reset_index(drop=True, inplace=True)\n",
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save main df \n",
    "main_df.to_csv(main_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup of main df\n",
    "#run every so often just so if we accidentally overwrite main_df save we still have our data\n",
    "import datetime\n",
    "\n",
    "filepath = './data/backup_my_dataframe_'+ str(datetime.datetime.now())+'.csv'\n",
    "main_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to save the state of the notebook so if I have to relaunch I don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill_session = '083018_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 11.8 ms, total: 128 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dill\n",
    "# Save\n",
    "dill.dump_session('project3_notebook_env_'+ dill_session +'.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 322 ms, sys: 76.8 ms, total: 399 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dill\n",
    "# Load\n",
    "dill.load_session('project3_notebook_env_' + dill_session + '.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the posts\n",
    "\n",
    "I need to make sure that the posts aren't giving away where they're from in any obvious way, such as having the subreddit name in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases that indicate subreddit name - to remove\n",
    "phrases = ['tales from tech support', 'looking for gamers', 'looking for games', 'no sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/r/lfg',\n",
       " '/r/nosleep',\n",
       " '/r/talesfromtechsupport',\n",
       " 'lfg',\n",
       " 'nosleep',\n",
       " 'r/lfg',\n",
       " 'r/nosleep',\n",
       " 'r/talesfromtechsupport',\n",
       " 'talesfromtechsupport',\n",
       " 'tfts',\n",
       " 'www.reddit.com/r/'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = set()\n",
    "for i in ['talesfromtechsupport', 'lfg', 'nosleep']:\n",
    "    words = [i, '/r/'+i, 'r/'+i, 'www.reddit.com/r/']\n",
    "    my_words.update(words)\n",
    "\n",
    "words = ['tfts']\n",
    "my_words.update(words)\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'exampl text entir sure paid $49.23 hey /r/dumb_th great place 39% comput comput comput wonder happen'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input\n",
    "    # text - string\n",
    "    # stem_or_lem - string, 'stem' uses PorterStemmer, 'lem' uses WordNetLemmatizer, anything else uses nothing\n",
    "    \n",
    "# Output\n",
    "    # a string\n",
    "    \n",
    "def process(text, stem_or_lem = 'lem', stop = 'english'):\n",
    "    \n",
    "    #Somewhere it was \n",
    "    try:\n",
    "        if len(text)==0:\n",
    "            return text\n",
    "    except:\n",
    "        return text\n",
    "    \n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove potential phrases of the subreddit name\n",
    "    phrases = ['tales from tech support', 'looking for gamers', 'looking for games', 'no sleep']\n",
    "    for i in phrases:\n",
    "        text = text.replace(i, '')\n",
    "\n",
    "    \n",
    "    # Grab all of the words. Disregard punctuation. \n",
    "    tokenizer = RegexpTokenizer(r'(\\$?(\\d+[\\.,]?)+%?|(\\/?\\w+)+)') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    #print(tokens)\n",
    "    new_tokens = [i[0] for i in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    if not stop==None:\n",
    "        #print('Stop')\n",
    "        stops = set(stopwords.words(stop))\n",
    "        stops.update(my_words)\n",
    "        new_tokens = [word for word in new_tokens if not word in stops]\n",
    "    \n",
    "    if stem_or_lem == 'lem':\n",
    "        #print('Lem')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        new_tokens = [lemmatizer.lemmatize(i) for i in new_tokens]\n",
    "        \n",
    "    elif stem_or_lem == 'stem':\n",
    "        #print('Stem')\n",
    "        p_stemmer = PorterStemmer()\n",
    "        new_tokens = [p_stemmer.stem(i) for i in new_tokens]\n",
    "        \n",
    "   \n",
    "    \n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "text = \"This is an example of some text! I'm NoT eNtIrElY sure why I paid $49.23 for it but hey, /r/dumb_thing /r/lfg r/LFG tales from tech support tfts looking for gamers is a great place. 39%. computers compute computing. I.Wonder.What'll.Happen\"\n",
    "\n",
    "results = process(text, 'stem')\n",
    "print(type(results))\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(process(None, 'stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(process(np.nan, 'stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit          object\n",
       "url                object\n",
       "author             object\n",
       "domain             object\n",
       "downs               int64\n",
       "is_self              bool\n",
       "is_video             bool\n",
       "likes             float64\n",
       "media             float64\n",
       "num_comments        int64\n",
       "num_crossposts      int64\n",
       "num_reports       float64\n",
       "selftext           object\n",
       "score               int64\n",
       "title              object\n",
       "ups                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df['selftext'] = main_df['selftext'].replace(np.nan, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subreddit', 'url', 'author', 'domain', 'downs', 'is_self', 'is_video',\n",
      "       'likes', 'media', 'num_comments', 'num_crossposts', 'num_reports',\n",
      "       'selftext', 'score', 'title', 'ups'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(main_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = main_df[(main_df['subreddit']=='talesfromtechsupport')|(main_df['subreddit']=='nosleep')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965, 16)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['title', 'selftext']:\n",
    "    working_df[i] = working_df[i].map(lambda x: process(x, 'lem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df['subreddit'] = working_df['subreddit'].map({'talesfromtechsupport':1, 'nosleep':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>MagicBigfoot</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hey two stickies something like 90% mod remova...</td>\n",
       "      <td>1958</td>\n",
       "      <td>posting rule mobile user please read</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>ArmaSwiss</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>work telecom data/phones/cctv dental office cl...</td>\n",
       "      <td>879</td>\n",
       "      <td>amp took client internet busy time 45 minute</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>kanersps</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hello everyone posted already r/choosingbegger...</td>\n",
       "      <td>31</td>\n",
       "      <td>free laptop good enough</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>Mr_White119811</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened whilst served armed force whilst spen...</td>\n",
       "      <td>122</td>\n",
       "      <td>clusterf k sandpit</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>sambeaux45</td>\n",
       "      <td>self.talesfromtechsupport</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best anonymize story best amp x200b tale go ba...</td>\n",
       "      <td>1944</td>\n",
       "      <td>time providing hour support may saved life</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                                url  \\\n",
       "0          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "4          1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "           author                     domain  downs  is_self  is_video  likes  \\\n",
       "0    MagicBigfoot  self.talesfromtechsupport      0     True     False    NaN   \n",
       "1       ArmaSwiss  self.talesfromtechsupport      0     True     False    NaN   \n",
       "2        kanersps  self.talesfromtechsupport      0     True     False    NaN   \n",
       "3  Mr_White119811  self.talesfromtechsupport      0     True     False    NaN   \n",
       "4      sambeaux45  self.talesfromtechsupport      0     True     False    NaN   \n",
       "\n",
       "   media  num_comments  num_crossposts  num_reports  \\\n",
       "0    NaN            77               0          NaN   \n",
       "1    NaN            91               0          NaN   \n",
       "2    NaN             5               0          NaN   \n",
       "3    NaN            15               0          NaN   \n",
       "4    NaN           202               0          NaN   \n",
       "\n",
       "                                            selftext  score  \\\n",
       "0  hey two stickies something like 90% mod remova...   1958   \n",
       "1  work telecom data/phones/cctv dental office cl...    879   \n",
       "2  hello everyone posted already r/choosingbegger...     31   \n",
       "3  happened whilst served armed force whilst spen...    122   \n",
       "4  best anonymize story best amp x200b tale go ba...   1944   \n",
       "\n",
       "                                          title   ups  \n",
       "0          posting rule mobile user please read  1958  \n",
       "1  amp took client internet busy time 45 minute   879  \n",
       "2                       free laptop good enough    31  \n",
       "3                            clusterf k sandpit   122  \n",
       "4    time providing hour support may saved life  1944  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = working_df['subreddit']\n",
    "X = working_df[['title', 'selftext', 'score', 'downs', 'ups', 'num_comments']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['both_texts'] = X['title'] + ' ' + X['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ended up using just the title and body of the post, all in 1 string, as the X. I might add in the rest of the numeric features at a later point\n",
    "X_train, X_test, y_train, y_test = train_test_split(X['both_texts'], y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "X_train_trans = cv.fit_transform(X_train)\n",
    "X_test_trans = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 5.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.3, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=5,\n",
       "          penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.3, penalty='l1', n_jobs=5, random_state=42)\n",
    "lr.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991869918699187"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_trans, y_test) \n",
    "# That's uh .. that's pretty good.\n",
    "# Makes me feel like there are some key words I might've missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 ngrams = 0.9939024\n",
    "1 ngrams = 0.9939024390243902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.11811282e-02, 9.18818872e-01],\n",
       "       [9.99466700e-01, 5.33300399e-04],\n",
       "       [8.18843606e-05, 9.99918116e-01],\n",
       "       [1.38875817e-02, 9.86112418e-01],\n",
       "       [1.90617537e-03, 9.98093825e-01],\n",
       "       [7.06948353e-04, 9.99293052e-01],\n",
       "       [9.49273598e-06, 9.99990507e-01],\n",
       "       [1.75516347e-05, 9.99982448e-01],\n",
       "       [5.71419753e-06, 9.99994286e-01],\n",
       "       [4.10675515e-04, 9.99589324e-01]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X_test_trans)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({'Features':cv.get_feature_names(),\n",
    "                        'coef':lr.coef_[0],\n",
    "                        'Absolute Coefficient':np.abs(lr.coef_[0])})\n",
    "\n",
    "temp_df.sort_values('Absolute Coefficient', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>coef</th>\n",
       "      <th>Absolute Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2039814</th>\n",
       "      <td>tech</td>\n",
       "      <td>0.718235</td>\n",
       "      <td>0.718235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624101</th>\n",
       "      <td>eye</td>\n",
       "      <td>-0.614272</td>\n",
       "      <td>0.614272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210946</th>\n",
       "      <td>user</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.582692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997257</th>\n",
       "      <td>support</td>\n",
       "      <td>0.571645</td>\n",
       "      <td>0.571645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559910</th>\n",
       "      <td>problem</td>\n",
       "      <td>0.565974</td>\n",
       "      <td>0.565974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370434</th>\n",
       "      <td>computer</td>\n",
       "      <td>0.520876</td>\n",
       "      <td>0.520876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128139</th>\n",
       "      <td>life</td>\n",
       "      <td>-0.514939</td>\n",
       "      <td>0.514939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671040</th>\n",
       "      <td>felt</td>\n",
       "      <td>-0.484610</td>\n",
       "      <td>0.484610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198167</th>\n",
       "      <td>body</td>\n",
       "      <td>-0.429625</td>\n",
       "      <td>0.429625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432529</th>\n",
       "      <td>customer</td>\n",
       "      <td>0.421375</td>\n",
       "      <td>0.421375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738375</th>\n",
       "      <td>saw</td>\n",
       "      <td>-0.415371</td>\n",
       "      <td>0.415371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2337664</th>\n",
       "      <td>working</td>\n",
       "      <td>0.397536</td>\n",
       "      <td>0.397536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016139</th>\n",
       "      <td>issue</td>\n",
       "      <td>0.391863</td>\n",
       "      <td>0.391863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058533</th>\n",
       "      <td>th</td>\n",
       "      <td>-0.343450</td>\n",
       "      <td>0.343450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790281</th>\n",
       "      <td>server</td>\n",
       "      <td>0.341453</td>\n",
       "      <td>0.341453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191431</th>\n",
       "      <td>blood</td>\n",
       "      <td>-0.336208</td>\n",
       "      <td>0.336208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138571</th>\n",
       "      <td>like</td>\n",
       "      <td>-0.331687</td>\n",
       "      <td>0.331687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367333</th>\n",
       "      <td>night</td>\n",
       "      <td>-0.324704</td>\n",
       "      <td>0.324704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.313254</td>\n",
       "      <td>0.313254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952637</th>\n",
       "      <td>house</td>\n",
       "      <td>-0.302528</td>\n",
       "      <td>0.302528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Features      coef  Absolute Coefficient\n",
       "2039814      tech  0.718235              0.718235\n",
       "624101        eye -0.614272              0.614272\n",
       "2210946      user  0.582692              0.582692\n",
       "1997257   support  0.571645              0.571645\n",
       "1559910   problem  0.565974              0.565974\n",
       "370434   computer  0.520876              0.520876\n",
       "1128139      life -0.514939              0.514939\n",
       "671040       felt -0.484610              0.484610\n",
       "198167       body -0.429625              0.429625\n",
       "432529   customer  0.421375              0.421375\n",
       "1738375       saw -0.415371              0.415371\n",
       "2337664   working  0.397536              0.397536\n",
       "1016139     issue  0.391863              0.391863\n",
       "2058533        th -0.343450              0.343450\n",
       "1790281    server  0.341453              0.341453\n",
       "191431      blood -0.336208              0.336208\n",
       "1138571      like -0.331687              0.331687\n",
       "1367333     night -0.324704              0.324704\n",
       "6166           18 -0.313254              0.313254\n",
       "952637      house -0.302528              0.302528"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df[temp_df['Absolute Coefficient']>0].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_l in ['stem', 'lem']:\n",
    "    MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "ddbc6159-6854-4ca7-857f-bfecdaf6d9c2"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
